{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911b3b37-3b29-4833-94f2-bfe47af00c83",
   "metadata": {},
   "source": [
    "# Self-Reflection: Error Detection and Correction in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd02f53-296c-4a1a-ab41-d46fa478ecc4",
   "metadata": {},
   "source": [
    "Self-reflection in AI is when the system looks at its own thinking or decision-making process to improve or adjust its responses.\n",
    "\n",
    "This example demonstrates the following process:\n",
    "1. The model generates an initial code response.\n",
    "2. It evaluates the quality of the generated code.\n",
    "3. Using feedback and suggestions, the model rewrites the code to improve its quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401647e8",
   "metadata": {
    "height": 523
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      " Here is a Python function that calculates the Fibonacci sequence up to n terms:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    for i in range(2, n):\n",
      "        next_num = fib_sequence[i-1] + fib_sequence[i-2]\n",
      "        fib_sequence.append(next_num)\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "# Test the function\n",
      "n = 10\n",
      "result = fibonacci(n)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "You can call this function with the desired number of terms (n) to get the Fibonacci sequence up to n terms.\n",
      "\n",
      "Debug Reflection:\n",
      " The code provided is correct and efficient for calculating the Fibonacci sequence up to n terms. However, there is a small issue in the code that can be improved for readability.\n",
      "\n",
      "In the `fibonacci` function, the initial Fibonacci sequence `[0, 1]` is hardcoded. It would be more readable and flexible to generate the initial sequence dynamically based on the first two numbers of the Fibonacci sequence. Here is an updated version of the function:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    for i in range(2, n):\n",
      "        next_num = fib_sequence[i-1] + fib_sequence[i-2]\n",
      "        fib_sequence.append(next_num)\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "# Test the function\n",
      "n = 10\n",
      "result = fibonacci(n)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "This change makes the code more readable and allows for easy modification of the initial sequence if needed. Other than that, the code is correct and efficient for calculating the Fibonacci sequence up to n terms.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def self_reflection_debug():\n",
    "    # Step 1: Generate code\n",
    "    code_prompt = \"Write a Python function to calculate the Fibonacci sequence up to n terms.\"\n",
    "    code_response = model.predict_messages([\n",
    "        SystemMessage(content=\"You are a coding assistant.\"),\n",
    "        HumanMessage(content=code_prompt)\n",
    "    ]).content\n",
    "\n",
    "    # Step 2: Reflect on potential errors or improvements\n",
    "    debug_prompt = (\n",
    "        f\"Here is your code:\\n\\n{code_response}\\n\\n\"\n",
    "        \"Evaluate this code for correctness, efficiency, and readability. \"\n",
    "        \"Suggest any necessary corrections or enhancements.\"\n",
    "    )\n",
    "    debug_response = model.predict_messages([\n",
    "        SystemMessage(content=\"You are a debugging assistant.\"),\n",
    "        HumanMessage(content=debug_prompt)\n",
    "    ]).content\n",
    "\n",
    "    return code_response, debug_response\n",
    "\n",
    "code, debug = self_reflection_debug()\n",
    "print(\"Generated Code:\\n\", code)\n",
    "print(\"\\nDebug Reflection:\\n\", debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776ba5e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
